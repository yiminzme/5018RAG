{
    // Use IntelliSense to learn about possible attributes.
    // Hover to view descriptions of existing attributes.
    // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python Debugger: Current File",
            "type": "debugpy",
            "request": "launch",
            "program": "${file}",
            "console": "integratedTerminal"
        },
        {
            // FROM run_compute_corpus_embeddings.sh
            // python src/compute_corpus_embeddings.py \
            // --corpus_path data/corpus.json \
            // --encoder_id facebook/contriever \
            // --max_length_encoder 512 \
            // --output_dir data/corpus/embeddings/wiki_dec_2018 \
            // --prefix_name contriever \
            // --batch_size 512 \
            // --save_every 500 \
            "name": "1.1 Compute Corpus Embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/compute_corpus_embeddings.py",
            "args": [
                "--corpus_path", "data/corpus.json",
                "--encoder_id", "facebook/contriever",
                "--max_length_encoder", "512",
                "--output_dir", "data/corpus/embeddings/wiki_dec_2018",
                "--prefix_name", "contriever",
                "--batch_size", "512",
                "--save_every", "500"
            ],
            "console": "integratedTerminal"
        },
        {
            // FROM run_indexing.sh
            // python src/index_embeddings.py \
            // --corpus_size 21035236 \
            // --vector_sz 768 \
            // --idx_type IP \
            // --faiss_dir data/corpus/faiss/wiki_dec_2018/to_gpu \
            // --percentages_for_index_splitting 60 \
            // --output_dir data/corpus/embeddings/wiki_dec_2018 \
            // --prefix_name contriever \
            // --batch_size 512 \
            // --save_every 500
            "name": "1.2 Index Embeddings",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/index_embeddings.py",
            "args": [
                "--corpus_size", "100",
                "--vector_sz", "768",
                "--idx_type", "IP",
                "--faiss_dir", "data/corpus/faiss/wiki_dec_2018/to_gpu",
                "--output_dir", "data/corpus/embeddings/wiki_dec_2018",
                "--prefix_name", "contriever",
                "--batch_size", "512",
                "--save_every", "500"
            ],
        },
        {
            // FROM run_search.sh
            // python src/compute_search_results.py \
            // --faiss_dir data/corpus/faiss/wiki_dec_2018/to_gpu \
            // --use_index_on_gpu True  \
            // --gpu_ids 0 1 \
            // --vector_sz 768 \
            // --encoder_id facebook/contriever \
            // --max_length_encoder 512 \
            // --top_docs 150 \
            // --use_test True \
            // --output_dir data/search_results \
            // --prefix_name contriever \
            // --batch_size 512 \
            // --index_batch_size 8
            "name": "1.3 Compute Search Results",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/compute_search_results.py",
            "args": [
                "--faiss_dir", "data/corpus/faiss/wiki_dec_2018/to_gpu",
                "--use_index_on_gpu", "False",
                "--vector_sz", "768",
                "--encoder_id", "facebook/contriever",
                "--max_length_encoder", "512",
                "--top_docs", "150",
                "--use_test", "True",
                "--output_dir", "data/search_results",
                "--prefix_name", "contriever",
                "--batch_size", "512",
                "--index_batch_size", "8"
            ],

        },
        {
            // FROM run_generation_only_query.sh
            // CUDA_VISIBLE_DEVICES=0 python src/generate_answers_llm_only_query.py \
            // --output_dir data/gen_res \
            // --llm_id meta-llama/Llama-2-7b-chat-hf \
            // --model_max_length 4096 \
            // --use_test False \
            // --batch_size 12 \
            // --save_every 250
            "name": "2.1 Generation w/o RAG",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/generate_answers_llm_only_query.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--model_max_length", "4096",
                "--use_test", "False",
                "--batch_size", "12",
                "--save_every", "250",
            ],
        },
        {
            // FROM run_generation.sh
            // CUDA_VISIBLE_DEVICES=0 python src/generate_answers_llm.py \
            // --output_dir data/gen_res \
            // --llm_id meta-llama/Llama-2-7b-chat-hf \
            // --model_max_length 4096 \
            // --load_full_corpus False \
            // --use_random True \
            // --use_adore False \
            // --gold_position 0 \
            // --num_documents_in_context 2 \
            // --get_documents_without_answer True \
            // --batch_size 18 \
            // --save_every 250
            "name": "2.2 Generation w/ RAG",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/generate_answers_llm.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--model_max_length", "4096", 
                "--load_full_corpus", "False",
                "--use_random", "False",
                "--use_adore", "False",
                "--gold_position", "3",
                "--num_documents_in_context", "7",
                "--get_documents_without_answer", "True",
                "--batch_size", "18",
                "--save_every", "250",
            ],
            "justMyCode": false,
        },
        {
            "name": "2.3 Generation w/ improved RAG",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/generate_answers_llm_improved.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--model_max_length", "4096", 
                "--load_full_corpus", "False",
                "--use_random", "False",
                "--use_adore", "False",
                "--gold_position", "3",
                "--num_documents_in_context", "7",
                "--get_documents_without_answer", "True",
                "--batch_size", "18",
                "--save_every", "250",
            ],
            "justMyCode": false,
        },
        {
            // FROM run_read_gen_res_only_query.sh
            // python src/read_generation_results.py \
            // --output_dir data/gen_res \
            // --llm_id meta-llama/Llama-2-7b-chat-hf \
            // --use_test False \
            // --prompt_type only_query
            "name": "3.1 Evaluate w/o RAG Result",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/read_generation_results.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--use_test", "False",
                "--prompt_type", "only_query"
            ],
        },
        {
            // python src/read_generation_results.py \
            // --output_dir data/gen_res \
            // --llm_id meta-llama/Llama-2-7b-chat-hf \
            // --use_test False \
            // --prompt_type classic \
            // --use_random True \
            // --use_adore False \
            // --gold_position 0 \
            // --num_documents_in_context 1 \
            // --get_documents_without_answer True \
            "name": "3.2 Evaluate w/ RAG Result",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/read_generation_results.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--use_test", "False",
                "--prompt_type", "classic",
                "--use_random", "False",
                "--use_adore", "False",
                "--gold_position", "3",
                "--num_documents_in_context", "7",
                "--get_documents_without_answer", "True"
            ],
        },
        {
            "name": "3.3 Evaluate w/ improved RAG Result",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/src/read_generation_results_improved.py",
            "args": [
                "--output_dir", "data/gen_res",
                "--llm_id", "meta-llama/Llama-2-7b-chat-hf",
                "--use_test", "False",
                "--prompt_type", "improved",
                "--use_random", "False",
                "--use_adore", "False",
                "--gold_position", "3",
                "--num_documents_in_context", "7",
                "--get_documents_without_answer", "True"
            ],
        }
    ]
}