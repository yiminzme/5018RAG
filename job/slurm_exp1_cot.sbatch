#!/bin/bash
#SBATCH --job-name=test          # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --gpus=1                 # number of GPUs per node(only valid under large/normal partition)
#SBATCH --time=07:20:00          # total run time limit (HH:MM:SS)
#SBATCH --partition=normal       # partition(large/normal/cpu) where you submit
#SBATCH --account=mscbdt2024     # only require for multiple projects

python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 3     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 4     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 5     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 6     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 8     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 9     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 10     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_100out     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 11     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
