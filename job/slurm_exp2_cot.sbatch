#!/bin/bash
#SBATCH --job-name=test          # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --gpus=1                 # number of GPUs per node(only valid under large/normal partition)
#SBATCH --time=09:20:00          # total run time limit (HH:MM:SS)
#SBATCH --partition=normal       # partition(large/normal/cpu) where you submit
#SBATCH --account=mscbdt2024     # only require for multiple projects

python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.1 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.2 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_3     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.3 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_4     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.4 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.5 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_6     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.6 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_7     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.7 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_8     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.8 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr0_9     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 0.9 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_thr1_0     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --filtering_threshold 1 --max_dataset_size 154 
