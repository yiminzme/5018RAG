#!/bin/bash
#SBATCH --job-name=test          # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --gpus=1                 # number of GPUs per node(only valid under large/normal partition)
#SBATCH --time=09:00:00          # total run time limit (HH:MM:SS)
#SBATCH --partition=normal       # partition(large/normal/cpu) where you submit
#SBATCH --account=mscbdt2024     # only require for multiple projects

python src/generate_answers_llm.py     --output_dir data/100out_baseline/3doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 3     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/4doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 4     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/5doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 5     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/6doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 6     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/8doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 8     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/9doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 9     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/10doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 10     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/11doc       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 11     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 

python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.1 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_3     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.3 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_4     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.4 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.5 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_6     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.6 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_7     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.7 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_8     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.8 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr0_9     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 0.9 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/thr1_0     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --filtering_threshold 1.0 --max_dataset_size 154 

python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold0       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 0     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold1       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold2       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold3       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold4       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold5       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/100out_baseline/7doc_gold6       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 6     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_new_tokens 100 --max_dataset_size 154 
