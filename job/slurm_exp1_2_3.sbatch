#!/bin/bash
#SBATCH --job-name=test          # create a short name for your job
#SBATCH --nodes=1                # node count
#SBATCH --gpus=1                 # number of GPUs per node(only valid under large/normal partition)
#SBATCH --time=09:20:00          # total run time limit (HH:MM:SS)
#SBATCH --partition=normal       # partition(large/normal/cpu) where you submit
#SBATCH --account=mscbdt2024     # only require for multiple projects

python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0       --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.1 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.2 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_3     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.3 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.5 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_6     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.6 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_8     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.8 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_thr0_9     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --filtering_threshold 0.9 --max_dataset_size 154 

python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 4     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 5     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 3     --num_documents_in_context 6     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 8     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 9     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 10     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 

python src/generate_answers_llm.py     --output_dir data/gen_res_rag_gold1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag_gold2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag_gold3     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm.py     --output_dir data/gen_res_rag_gold5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 

python src/generate_answers_llm_improved.py     --output_dir data/gen_res_ourrag_gold1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_ourrag_gold2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_ourrag_gold3     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_ourrag_gold5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --max_dataset_size 154 

python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold0     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 0     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold1     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 1     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold2     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 2     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold4     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 4     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold5     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 5     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 
python src/generate_answers_llm_improved.py     --output_dir data/gen_res_cot_gold6     --llm_id meta-llama/Llama-2-7b-chat-hf     --model_max_length 4096     --load_full_corpus False     --use_random False     --use_adore False     --gold_position 6     --num_documents_in_context 7     --get_documents_without_answer True     --batch_size 18     --save_every 2 --cot True --max_new_tokens 100 --max_dataset_size 154 